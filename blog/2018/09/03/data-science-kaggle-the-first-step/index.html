
<!DOCTYPE HTML>

<html>

<head>
	<meta charset="utf-8">
	<title>Data Science - Kaggle - First step - 陋室</title>
	<meta name="author" content="Wanax">
	
	<meta name="description" content="Data Science - Kaggle - First Step This is my first blog written by English, you know&hellip;.. A. Kaggle Here is the wiki link, talks about what &hellip;">
	

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	<link href="/atom.xml" rel="alternate" title="陋室" type="application/atom+xml">
	
	<link rel="canonical" href="http://sonnewilling.com/blog/2018/09/03/data-science-kaggle-the-first-step/">
	<link href="/favicon.png" rel="shortcut icon">
	<link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<link href="/stylesheets/font-awesome.min.css" media="screen, projection" rel="stylesheet" type="text/css">
	<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
<!-- 	<link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,400,700' rel='stylesheet' type='text/css'> -->
	<script src="/javascripts/jquery-1.9.1.js"></script>
	<script type="text/javascript" src="http://tajs.qq.com/stats?sId=31767964" charset="UTF-8"></script>
	<!--Fonts from Google"s Web font directory at http://google.com/webfonts 
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
-->
<script type="text/javascript">
	function addBlankTargetForLinks () {
	  $('a[href^="http"]').each(function(){
	      $(this).attr('target', '_blank');
	  });
	}

	$(document).bind('DOMNodeInserted', function(event) {
	  addBlankTargetForLinks();
	});
</script>
  

</head>


<body>
	<div class="container">
		<div class="left-col">
			<div class="intrude-less">
			<header id="header" class="inner"><div class="profilepic">
	<img src='/images/avatar.png' alt='Profile Picture' style='width: 160px;' />
</div>
<hgroup>
  <h1><a href="/">陋室</a></h1>
  
    <h2 class="subtitle">碌碌二十余载,自思无益于国家,每念及此,万念俱灰</h2>
  
</hgroup>

<nav id="main-nav"><ul class="main-navigation">
  <!--<li><a href="/">首页</a></li>-->
  <li><a href="/blog/archives">所有文章</a></li>
  <li><a href="/intro/index.html">关于我</a></li>
</ul>
<section>
  <h3 class="catag">文章类别</h3>
  <ul id="categories">
    <li class='category'><a href='/blog/categories/life/'>Life (20)</a></li>
<li class='category'><a href='/blog/categories/ontheroad/'>OnTheRoad (13)</a></li>
<li class='category'><a href='/blog/categories/read/'>Read (4)</a></li>
<li class='category'><a href='/blog/categories/tec/'>Tec (33)</a></li>

  </ul>
</section>

</nav>
<nav id="sub-nav">
	<div class="social">
		
		<a class="weibo" href="http://www.weibo.com/1829617555" title="Weibo">Weibo</a>
		
		
		
		
		
		<a class="github" href="https://github.com/wanax" title="GitHub">GitHub</a>
		
		
		
		
		
		
		
		
		<a class="rss" href="/atom.xml" title="RSS">RSS</a>
		
	</div>
</nav>
</header>				
			</div>
		</div>	
		<div class="mid-col">
			
				
			
			<div class="mid-col-container">
				<div id="content" class="inner"><article class="post" itemscope itemtype="http://schema.org/BlogPosting">
	<h1 class="title" itemprop="name">Data Science - Kaggle - First Step</h1>
	<div class="entry-content" itemprop="articleBody"><blockquote><p>This is my first blog written by English, you know&hellip;..</p></blockquote>

<h3>A. <a href="https://www.kaggle.com/">Kaggle</a></h3>

<p>Here is the wiki <a href="https://en.wikipedia.org/wiki/Kaggle">link</a>, talks about what Kaggle is and what can we get.</p>

<h3>B. <a href="https://www.kaggle.com/c/titanic">Titanic: Machine Learning from Disaster</a></h3>

<p>The <strong>Titanic Disaster</strong> is almost every fresh bird&rsquo;s first lesson to unveil the Kaggle&rsquo;s veil (Yes, &ldquo;Unveil the veil&rdquo;, I get it from google translation).</p>

<p>Basically,  <strong>Titanic</strong> gives us an Excel called <code>train.csv</code> which contains passengers&#8217; information such as name, sex, age and so on. The <code>train.csv</code> has an important column called <strong>Survived</strong>, which is our task to analysis the relationship between <strong>Survived</strong> and the other columns. And there is another file called <code>test.csv</code>, it&rsquo;s columns almost as same as <code>train.csv</code> except it does not contain <strong>Survived</strong>. As participants, we should build a model from <code>train.csv</code> and use this model to predict each passenger which in the <code>test.csv</code>  is alive or not. The more accurate you predict, the better grade you will get.</p>

<!-- more -->


<h3>C. The normal process</h3>

<ul>
<li><p>Business Understanding</p></li>
<li><p>Data Understanding</p></li>
<li><p>Data Preparation</p></li>
<li><p>Modeling</p></li>
<li><p>Evaluation</p></li>
<li><p>Deployment</p></li>
</ul>


<p><strong>1. Business Understanding</strong></p>

<p>Just like Ch B I talk about, every competition has its own targets, we should know exactly what the purpose is.</p>

<p><strong>2.Data Understanding</strong></p>

<p>Each <code>train.csv</code> contains lots of columns, some of them may very meaningful and the others may be just some smoke bomb (Do they know that&rsquo;s mean?). The first step is find out which attribute actually affect the target such as <strong>Survived</strong>.</p>

<p>Take <strong>Titanic </strong> as an example, first, we load the data:</p>

<p><code>data_train = pd.read_csv(r'D:\house\project\python\test\train.csv')</code></p>

<p><img src="/images/tec/kaggle01/form01.png" alt="image" /></p>

<p>As you can see, there are lots of passengers and each passenger has many attributes, some of them&rsquo;s <strong>Survived</strong>=0 means they were dead, on the contrary(I always use this phrase in my English writing),   <strong>Survived</strong>=1 means they made alive.</p>

<p>For intuition, I believe the ticket class which means rich or poor, age and sex will strong affect survived rata. Let me check this.</p>

<p>First, the ticket class:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Survived_0 = data_train.Pclass[data_train.Survived == 0].value_counts()
</span><span class='line'>Survived_1 = data_train.Pclass[data_train.Survived == 1].value_counts()
</span><span class='line'>
</span><span class='line'>df = pd.DataFrame({'Survived':Survived_1, 'Dead':Survived_0})
</span><span class='line'>df.plot(kind='bar', stacked=True)</span></code></pre></td></tr></table></div></figure>


<p>   <img src="/images/tec/kaggle01/class.png" alt="image" /></p>

<p>   opps, the higher ticket class, the more chance for people to make alive. Wealth indeed is an important attribute.</p>

<p>   Second, sex:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Survived_m = data_train.Survived[data_train.Sex == 'male'].value_counts()
</span><span class='line'>Survived_f = data_train.Survived[data_train.Sex == 'female'].value_counts()
</span><span class='line'>df2 = pd.DataFrame({'male':Survived_m, 'female':Survived_f})
</span><span class='line'>df2.plot(kind='bar', stacked=True)</span></code></pre></td></tr></table></div></figure>


<p>   <img src="/images/tec/kaggle01/sex.png" alt="image" /></p>

<p>   Yes, women have more chance than men to make alive.</p>

<p>   Last, age, let us make some different:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>plt.subplot2grid((2,3),(1,0), colspan=2)
</span><span class='line'>data_train.Age[data_train.Pclass == 1].plot(kind='kde')
</span><span class='line'>data_train.Age[data_train.Pclass == 2].plot(kind='kde')
</span><span class='line'>data_train.Age[data_train.Pclass == 3].plot(kind='kde')
</span><span class='line'>plt.xlabel('Age')
</span><span class='line'>plt.ylabel('Con')
</span><span class='line'>plt.title('Each class age dis')
</span><span class='line'>plt.legend(('1st','2st','3st'),loc='best')</span></code></pre></td></tr></table></div></figure>


<p>   <img src="/images/tec/kaggle01/age.png" alt="image" /></p>

<p>   Above code shows older people live in higher class cabin which also means the relationship between age and wealth.</p>

<p>   Again, Data Understanding is a very important step, my demo code is too simple to show how to figure out these relations, you can get a clearly instruction from  <a href="https://www.kaggle.com/helgejo/an-interactive-data-science-tutorial">An Interactive Data Science Tutorial</a>.</p>

<p>   <strong>3. Data Preparation</strong></p>

<p>   Before build prediction model, we should rearrange our data. In fact,  we may face many problems about the <code>train.csv</code>. Such as missing or the data type is string which most machine learning models in Python can not handle them very well. Here are some way to solve these problems</p>

<ul>
<li><p> Drop</p>

<p> If one attribute lack most of data, like only one of ten has value, you can just drop it away.</p></li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>if X_train[X_train.Age.isnull()].value_counts() &gt; max:
</span><span class='line'> reduced_X_train = X_train.drop(X_train.Age, axis=1)</span></code></pre></td></tr></table></div></figure>


<ul>
<li><p> Set Yes or No</p>

<p> If attribute which you believe is important however it has half and half missing value,  you can set <strong>Yes</strong> or <strong>No</strong> for value or null. Like this:</p></li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>df.loc[(df.Cabin.notnull()), 'Cabin'] = "Yes"
</span><span class='line'>df.loc[(df.Cabin.isnull()), 'Cabin'] = "No"</span></code></pre></td></tr></table></div></figure>


<ul>
<li><p> Impute</p>

<p>   <strong>KSLearn</strong> provides us an easy way to fill vacancy:</p></li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>##Get Model Score from Imputation
</span><span class='line'>from sklearn.preprocessing.imputation import Imputer
</span><span class='line'>my_imputer = Imputer()
</span><span class='line'>imputed_X_train = my_imputer.fit_transform(X_train)
</span><span class='line'>imputed_X_test = my_imputer.transform(X_test)</span></code></pre></td></tr></table></div></figure>


<ul>
<li><p>Using Categorical Data with One Hot Encoding</p>

<p>For example, if you people responded to a survey about which what brand of car they owned, the result would be categorical (because the answers would be things like <em>Honda</em>, <em>Toyota</em>, <em>Ford</em>, <em>None</em>, etc.). Responses fall into a fixed set of categories.</p>

<p>You will get an error if you try to plug these variables into most machine learning models in Python without &ldquo;encoding&rdquo; them first. Here we&rsquo;ll show the most popular method for encoding categorical variables.</p>

<p>  <img src="/images/tec/kaggle01/one-hot.png" alt="image" /></p></li>
</ul>


<p>   It&rsquo;s most common to one-hot encode these &ldquo;object&rdquo; columns, since they can&rsquo;t be plugged directly into most models. Pandas offers a convenient function called <strong>get_dummies</strong> to get one-hot encodings. Call it like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)</span></code></pre></td></tr></table></div></figure>


<p>To see detail about this, can view <a href="https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding">Using Categorical Data with One Hot Encoding</a></p>

<p>   <strong>4. Modeling</strong></p>

<p>   Finally, we move here&hellip;.</p>

<p>   After we have cleaned up data, we can choose one machine learning model to predict. I have just learned <strong>Decision Tree</strong> and <strong>Random Forest</strong>, and have heard the <strong>XGBoost</strong> is the leading model for working with standard tabular data <strong>XGBoost</strong> models dominate many <strong>Kaggle</strong> competitions.</p>

<p>   To use one model, the code like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>model = RandomForestRegressor()
</span><span class='line'>model.fit(X_train, y_train)
</span><span class='line'>preds = model.predict(X_test)</span></code></pre></td></tr></table></div></figure>


<p>   So easy, isn&rsquo;t it? The <em>preds</em> is our target which should produce an Excel file to submit to <strong>Kaggle</strong>.</p>

<p>   <strong>5. Evaluation</strong></p>

<p>   No one can make it perfect at once, we should optimize again and again.  The normal benchmark is called <strong>Mean Absolute Error</strong> (also called <strong>MAE</strong>).</p>

<p>   The prediction error for each passenger is:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>error=actual−predicted</span></code></pre></td></tr></table></div></figure>


<p>   It&rsquo;s easy to use like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>from sklearn.metrics import mean_absolute_error
</span><span class='line'>predicted_home_prices = melbourne_model.predict(X)
</span><span class='line'>mean_absolute_error(y, predicted_home_prices)</span></code></pre></td></tr></table></div></figure>


<p>   <strong>6. Deployment</strong></p>

<p>   Bla Bla Bla, finish!</p>

<h3>D. End</h3>

<p>OK, this is the simplest way to go through one competition and the result must be awful, lol. There are lot of detail I have not mentioned or learned, like the powerful model <strong>XSBoost</strong>.</p>

<p>Anyway, this is my first blog about data science, hope it will be a good beginning.</p>
</div>

</article>

	<!--<div class="share">
	<div class="addthis_toolbox addthis_default_style ">
	
	
	
	<a class="addthis_counter addthis_pill_style"></a>
	</div>
  <script type="text/javascript" src="http://s7.addthis.com/js/250/addthis_widget.js#pubid="></script>
</div>
-->


<section id="comment">
    <h1 class="title">Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</section>
</div>
			</div>
			<footer id="footer" class="inner"><p>
  Copyright &copy; 2018 - Wanax -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

Design credit: <a href="http://shashankmehta.in/archive/2012/greyshade.html">Shashank Mehta</a></footer>
			

<script type="text/javascript">
      var disqus_shortname = 'wanax';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://sonnewilling.com/blog/2018/09/03/data-science-kaggle-the-first-step/';
        var disqus_url = 'http://sonnewilling.com/blog/2018/09/03/data-science-kaggle-the-first-step/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>









<script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F6e9d609916419478e19b64e7fcbc720c' type='text/javascript'%3E%3C/script%3E"));
</script>

		</div>
	</div>
</body>
</html>
